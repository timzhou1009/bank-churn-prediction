{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commercial Bank Customer Retention Prediction\n",
    "\n",
    "## APSTA-GE.2401: Statistical Consulting\n",
    "\n",
    "## Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on: 12/08/2020\n",
    "\n",
    "Modified on: 12/09/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This script processes data from the proprocess step.\n",
    "\n",
    "### Data\n",
    "\n",
    "The data are preprocessed feature sets:\n",
    "\n",
    "  - `X_train.csv`: contains all features in Q3 and Q4 of 2019 for training. Imported as `X`.\n",
    "  - `y_train.csv`: contains the label variable for validation. Imported as `y`.\n",
    "  - `X_test.csv`: contains all features in Q1 of 2020 for testing. Imported as `X_true`.\n",
    "   \n",
    "After importing the data, we confirmed that both train sets have the same number of records: **145296**. We also confirmed that the testing set has **76722** records.\n",
    "\n",
    "### Procedures\n",
    "\n",
    "We first inspected the feature set. \n",
    "\n",
    "1. There are 55 features in the feature set. \n",
    "\n",
    "2. We checked if there are any missing values in the set. We found multiple columns that contain missing values, ranging from 0.005% to 100%. For columns containing a large portion of missing values, we dropped the column to reduce computational burden. For columns containing a small portion of missing values, we applied a deep learning library, [Datawig](https://github.com/awslabs/datawig), which learns machine learning models using deep neural networks to impute missing values in the data.\n",
    "\n",
    "    - After dropping columns containing large portion of missing values, we reduced number of features to 45.\n",
    "\n",
    "3. We then performed dummy coding to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! All modules are imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "\n",
    "print('SUCCESS! All modules are imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data/preprocess/X_train.csv')\n",
    "y = pd.read_csv('../data/preprocess/y_train.csv')\n",
    "X_true = pd.read_csv('../data/preprocess/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proprocessed training set has 145296 rows and 56 columns.\n",
      "The proprocessed validation set has 145296 rows and 2 columns.\n",
      "The proprocessed testing set has 76722 rows and 56 columns.\n"
     ]
    }
   ],
   "source": [
    "print('The proprocessed training set has {} rows and {} columns.'.format(X.shape[0], X.shape[1]))\n",
    "print('The proprocessed validation set has {} rows and {} columns.'.format(y.shape[0], y.shape[1]))\n",
    "print('The proprocessed testing set has {} rows and {} columns.'.format(X_true.shape[0], X_true.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing(dat):\n",
    "    '''Print missing values in each column of the dat\n",
    "    @Param df dat: input data frame\n",
    "    '''\n",
    "    missing_val = dat.isnull().sum()\n",
    "    for index in missing_val.index:\n",
    "        if missing_val[index] > 0:\n",
    "            print('{} has {} missing values. ({:.4%})'.format(index, missing_val[index], missing_val[index]/len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_datatime(dat, col, fmt):\n",
    "    '''Convert col in dat to float using day0 as the reference date\n",
    "    @Param df dat: input data frame\n",
    "    @Param str col: column name\n",
    "    @Param str fmt: date time format\n",
    "    '''\n",
    "    year = col + '_y'\n",
    "    month = col + '_m'\n",
    "    day = col + '_d'\n",
    "    \n",
    "    dat[year] = pd.DatetimeIndex(dat[col]).year\n",
    "    dat[month] = pd.DatetimeIndex(dat[col]).month\n",
    "    dat[day] = pd.DatetimeIndex(dat[col]).day\n",
    "    dat.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y (Label for Validation)\n",
    "\n",
    "We applied `LabelBinarizer` to make the label binary. Originally, the label column contains three values: \n",
    "- 1: indicating churn\n",
    "- 0: indicating no preference\n",
    "- -1: indicating not churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    92818\n",
       " 0    30237\n",
       "-1    22241\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('../data/preprocess/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "lb.fit(y['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = lb.transform(y['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X (Feature for Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "We first processed missing values in the data. Multiple columns contain missing values. The percentage of missing values in each column ranges from 0.0048% to 100.00%. We removed columns containing large portion of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B6 has 8878 missing values. (6.1103%)\n",
      "E2 has 6370 missing values. (4.3842%)\n",
      "E3 has 6370 missing values. (4.3842%)\n",
      "E4 has 84483 missing values. (58.1454%)\n",
      "E5 has 55129 missing values. (37.9425%)\n",
      "E6 has 7538 missing values. (5.1880%)\n",
      "E7 has 142402 missing values. (98.0082%)\n",
      "E8 has 127381 missing values. (87.6700%)\n",
      "E9 has 145227 missing values. (99.9525%)\n",
      "E10 has 816 missing values. (0.5616%)\n",
      "E11 has 145296 missing values. (100.0000%)\n",
      "E12 has 121324 missing values. (83.5013%)\n",
      "E13 has 127502 missing values. (87.7533%)\n",
      "E14 has 90010 missing values. (61.9494%)\n",
      "E16 has 68530 missing values. (47.1658%)\n",
      "E18 has 62147 missing values. (42.7727%)\n",
      "C1 has 7 missing values. (0.0048%)\n",
      "C2 has 7 missing values. (0.0048%)\n",
      "I1 has 64 missing values. (0.0440%)\n",
      "I5 has 11604 missing values. (7.9865%)\n",
      "I9 has 145296 missing values. (100.0000%)\n",
      "I10 has 128487 missing values. (88.4312%)\n",
      "I13 has 143108 missing values. (98.4941%)\n",
      "I14 has 129650 missing values. (89.2316%)\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "check_missing(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with large portion of missing values\n",
    "col_to_drop = ['E7', 'E8', 'E9', 'E11', 'E12', 'E13', 'I9', 'I10', 'I13', 'I14']\n",
    "X = X.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping columns containing large portion of missing values, now the set has 46 columns.\n"
     ]
    }
   ],
   "source": [
    "print('After dropping columns containing large portion of missing values, now the set has {} columns.'.format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B6 has 8878 missing values. (6.1103%)\n",
      "E2 has 6370 missing values. (4.3842%)\n",
      "E3 has 6370 missing values. (4.3842%)\n",
      "E4 has 84483 missing values. (58.1454%)\n",
      "E5 has 55129 missing values. (37.9425%)\n",
      "E6 has 7538 missing values. (5.1880%)\n",
      "E10 has 816 missing values. (0.5616%)\n",
      "E14 has 90010 missing values. (61.9494%)\n",
      "E16 has 68530 missing values. (47.1658%)\n",
      "E18 has 62147 missing values. (42.7727%)\n",
      "C1 has 7 missing values. (0.0048%)\n",
      "C2 has 7 missing values. (0.0048%)\n",
      "I1 has 64 missing values. (0.0440%)\n",
      "I5 has 11604 missing values. (7.9865%)\n"
     ]
    }
   ],
   "source": [
    "check_missing(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6: Latest transfer time\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "X['B6'] = pd.to_datetime(X['B6'], format=fmt, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['B6'] = X['B6'].replace(np.NaN, X['B6'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E category\n",
    "fmt = '%Y-%m-%d'\n",
    "col_names = ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E10', 'E14', 'E16', 'E18']\n",
    "for col_name in col_names:\n",
    "    X[col_name] = pd.to_datetime(X[col_name], format=fmt, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2: Online banking opening date\n",
    "X['E2'] = X['E2'].replace(np.NaN, X['E2'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E3: Mobile banking opening date\n",
    "X['E3'] = X['E3'].replace(np.NaN, X['E3'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4: First online banking login date\n",
    "X['E4'] = X['E4'].replace(np.NaN, X['E4'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5: First mobile banking login date\n",
    "X['E5'] = X['E5'].replace(np.NaN, X['E5'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E6: First demand deposit date\n",
    "X['E6'] = X['E6'].replace(np.NaN, X['E6'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E10: First cash transaction date\n",
    "X['E10'] = X['E10'].replace(np.NaN, X['E10'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E14: First transfer via mobile banking date\n",
    "X['E14'] = X['E14'].replace(np.NaN, X['E14'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E16: Maximum amount transferred out of another bank date\n",
    "X['E16'] = X['E16'].replace(np.NaN, X['E16'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E18: Maximum transfer amount from other bank date\n",
    "X['E18'] = X['E18'].replace(np.NaN, X['E18'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1\n",
    "X['C1'] = X['C1'].fillna(0)\n",
    "# C2\n",
    "X['C2'] = X['C2'].fillna(0)\n",
    "# I1\n",
    "X['I1'] = X['I1'].replace(np.NaN, '女性')\n",
    "# I5\n",
    "X['I5'] = X['I5'].replace(np.NaN, '未知')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Meaningless Columns\n",
    "\n",
    "Based on the codebook, after mining into the data, we determined that the following columns contain meaningless information and, therefore, we dropped these columns:\n",
    "\n",
    "- `I8`: constellation. We don't believe constellation can alter customer behavior.\n",
    "- `I12`: field description. Contain only 1 different values.\n",
    "- `I15`: QR code recipient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['I8', 'I12', 'I15']\n",
    "X = X.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping columns containing large portion of missing values, now the set has 43 columns.\n"
     ]
    }
   ],
   "source": [
    "print('After dropping columns containing large portion of missing values, now the set has {} columns.'.format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cust_no', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'B1', 'B2',\n",
       "       'B3', 'B4', 'B5', 'B6', 'B7', 'E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E10',\n",
       "       'E14', 'E15', 'E16', 'E17', 'E18', 'C1', 'C2', 'I1', 'I2', 'I3', 'I4',\n",
       "       'I5', 'I6', 'I7', 'I11', 'I16', 'I17', 'I18', 'I19', 'I20'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Coding\n",
    "\n",
    "We dummy coded categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Date Time Columns\n",
    "\n",
    "To dummy code columns containing date and time, we first converted all string-like inputs as date time format. Then, we created dummy columns for `year`, `month` and `day` of each datetime column. Finally, we dropped the original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6: Latest transfer time\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "code_datatime(X, 'B6', fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E category\n",
    "fmt = '%Y-%m-%d'\n",
    "col_names = ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E10', 'E14', 'E16', 'E18']\n",
    "for col_name in col_names:\n",
    "    code_datatime(X, col_name, fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145296, 76)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cust_no', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'B1', 'B2',\n",
       "       'B3', 'B4', 'B5', 'B6', 'B7', 'E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E10',\n",
       "       'E14', 'E15', 'E16', 'E17', 'E18', 'C1', 'C2', 'I1', 'I2', 'I3', 'I4',\n",
       "       'I5', 'I6', 'I7', 'I11', 'I16', 'I17', 'I18', 'I19', 'I20', 'B6_y',\n",
       "       'B6_m', 'B6_d', 'E1_y', 'E1_m', 'E1_d', 'E2_y', 'E2_m', 'E2_d', 'E3_y',\n",
       "       'E3_m', 'E3_d', 'E4_y', 'E4_m', 'E4_d', 'E5_y', 'E5_m', 'E5_d', 'E6_y',\n",
       "       'E6_m', 'E6_d', 'E10_y', 'E10_m', 'E10_d', 'E14_y', 'E14_m', 'E14_d',\n",
       "       'E16_y', 'E16_m', 'E16_d', 'E18_y', 'E18_m', 'E18_d'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Columns\n",
    "\n",
    "To prevent algorithms from interpreting hierarchy in dummy coded categorical columns, we applied `One-hot encoding` to categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I5: Occupation\n",
    "X['I5'] = pd.get_dummies(X['I5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col_names = ['I1', 'I3', 'I4', 'I6', 'I16', 'I17', 'I18', 'I19', 'I20']\n",
    "enc_df = pd.DataFrame(enc.fit_transform(X[cat_col_names]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = X.join(enc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = X_encoded.drop(cat_col_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dummy coding, now the set has 87 columns.\n"
     ]
    }
   ],
   "source": [
    "print('After dummy coding, now the set has {} columns.'.format(X_encoded.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145296, 87)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X_true (Features for Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Similar to the `X`, we first processed missing values in the data. Multiple columns contain missing values. The percentage of missing values in each column ranges from 0.0048% to 100.00%. We removed columns containing large portion of missing values.\n",
    "\n",
    "### Drop Meaningless Columns\n",
    "\n",
    "Based on the codebook, after mining into the data, we determined that the following columns contain meaningless information and, therefore, we dropped these columns:\n",
    "\n",
    "- `I8`: constellation. We don't believe constellation can alter customer behavior.\n",
    "- `I12`: field description. Contain only 1 different values.\n",
    "- `I15`: QR code recipient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B6 has 13429 missing values. (9.2425%)\n",
      "E2 has 2889 missing values. (1.9884%)\n",
      "E3 has 2889 missing values. (1.9884%)\n",
      "E4 has 44687 missing values. (30.7558%)\n",
      "E5 has 28121 missing values. (19.3543%)\n",
      "E6 has 1799 missing values. (1.2382%)\n",
      "E7 has 74289 missing values. (51.1294%)\n",
      "E8 has 67269 missing values. (46.2979%)\n",
      "E9 has 76619 missing values. (52.7330%)\n",
      "E10 has 589 missing values. (0.4054%)\n",
      "E11 has 76722 missing values. (52.8039%)\n",
      "E12 has 62490 missing values. (43.0088%)\n",
      "E13 has 67701 missing values. (46.5952%)\n",
      "E14 has 45441 missing values. (31.2748%)\n",
      "E16 has 34876 missing values. (24.0034%)\n",
      "E18 has 32254 missing values. (22.1988%)\n",
      "C1 has 3 missing values. (0.0021%)\n",
      "C2 has 3 missing values. (0.0021%)\n",
      "I1 has 32 missing values. (0.0220%)\n",
      "I5 has 3795 missing values. (2.6119%)\n",
      "I9 has 76722 missing values. (52.8039%)\n",
      "I10 has 67797 missing values. (46.6613%)\n",
      "I13 has 74722 missing values. (51.4274%)\n",
      "I14 has 68320 missing values. (47.0213%)\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "check_missing(X_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with large portion of missing values\n",
    "col_to_drop = ['E7', 'E8', 'E9', 'E11', 'E12', 'E13', 'I9', 'I10', 'I13', 'I14',\n",
    "              'I8', 'I12', 'I15']\n",
    "X_true = X_true.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping columns containing large portion of missing values and meaningless columns, now the set has 43 columns.\n"
     ]
    }
   ],
   "source": [
    "print('After dropping columns containing large portion of missing values and meaningless columns, now the set has {} columns.'.format(X_true.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B6 has 13429 missing values. (9.2425%)\n",
      "E2 has 2889 missing values. (1.9884%)\n",
      "E3 has 2889 missing values. (1.9884%)\n",
      "E4 has 44687 missing values. (30.7558%)\n",
      "E5 has 28121 missing values. (19.3543%)\n",
      "E6 has 1799 missing values. (1.2382%)\n",
      "E10 has 589 missing values. (0.4054%)\n",
      "E14 has 45441 missing values. (31.2748%)\n",
      "E16 has 34876 missing values. (24.0034%)\n",
      "E18 has 32254 missing values. (22.1988%)\n",
      "C1 has 3 missing values. (0.0021%)\n",
      "C2 has 3 missing values. (0.0021%)\n",
      "I1 has 32 missing values. (0.0220%)\n",
      "I5 has 3795 missing values. (2.6119%)\n"
     ]
    }
   ],
   "source": [
    "check_missing(X_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6: Latest transfer time\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "X_true['B6'] = pd.to_datetime(X_true['B6'], format=fmt, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true['B6'] = X_true['B6'].replace(np.NaN, X_true['B6'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E category\n",
    "fmt = '%Y-%m-%d'\n",
    "col_names = ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E10', 'E14', 'E16', 'E18']\n",
    "for col_name in col_names:\n",
    "    X_true[col_name] = pd.to_datetime(X_true[col_name], format=fmt, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2: Online banking opening date\n",
    "X_true['E2'] = X_true['E2'].replace(np.NaN, X_true['E2'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E3: Mobile banking opening date\n",
    "X_true['E3'] = X_true['E3'].replace(np.NaN, X_true['E3'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4: First online banking login date\n",
    "X_true['E4'] = X_true['E4'].replace(np.NaN, X_true['E4'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5: First mobile banking login date\n",
    "X_true['E5'] = X_true['E5'].replace(np.NaN, X_true['E5'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E6: First demand deposit date\n",
    "X_true['E6'] = X_true['E6'].replace(np.NaN, X_true['E6'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E10: First cash transaction date\n",
    "X_true['E10'] = X_true['E10'].replace(np.NaN, X_true['E10'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E14: First transfer via mobile banking date\n",
    "X_true['E14'] = X_true['E14'].replace(np.NaN, X_true['E14'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E16: Maximum amount transferred out of another bank date\n",
    "X_true['E16'] = X_true['E16'].replace(np.NaN, X_true['E16'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E18: Maximum transfer amount from other bank date\n",
    "X_true['E18'] = X_true['E18'].replace(np.NaN, X_true['E18'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1\n",
    "X_true['C1'] = X_true['C1'].fillna(0)\n",
    "# C2\n",
    "X_true['C2'] = X_true['C2'].fillna(0)\n",
    "# I1\n",
    "X_true['I1'] = X_true['I1'].replace(np.NaN, '女性')\n",
    "# I5\n",
    "X_true['I5'] = X_true['I5'].replace(np.NaN, '未知')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing(X_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Coding\n",
    "\n",
    "Before applying `Datawig`, we dummy coded categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6: Latest transfer time\n",
    "fmt = '%Y-%m-%d %H:%M:%S'\n",
    "code_datatime(X_true, 'B6', fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E category\n",
    "fmt = '%Y-%m-%d'\n",
    "col_names = ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E10', 'E14', 'E16', 'E18']\n",
    "for col_name in col_names:\n",
    "    code_datatime(X_true, col_name, fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true['I5'] = pd.get_dummies(X_true['I5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = pd.DataFrame(enc.fit_transform(X_true[cat_col_names]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true_encoded = X_true.join(enc_df)\n",
    "X_true_encoded = X_true_encoded.drop(cat_col_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dummy coding, now the set has 87 columns.\n"
     ]
    }
   ],
   "source": [
    "print('After dummy coding, now the set has {} columns.'.format(X_true_encoded.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cust_no',      'X1',      'X2',      'X3',      'X4',      'X5',\n",
       "            'X6',      'X7',      'X8',      'B1',      'B2',      'B3',\n",
       "            'B4',      'B5',      'B6',      'B7',      'E1',      'E2',\n",
       "            'E3',      'E4',      'E5',      'E6',     'E10',     'E14',\n",
       "           'E15',     'E16',     'E17',     'E18',      'C1',      'C2',\n",
       "            'I2',      'I5',      'I7',     'I11',    'B6_y',    'B6_m',\n",
       "          'B6_d',    'E1_y',    'E1_m',    'E1_d',    'E2_y',    'E2_m',\n",
       "          'E2_d',    'E3_y',    'E3_m',    'E3_d',    'E4_y',    'E4_m',\n",
       "          'E4_d',    'E5_y',    'E5_m',    'E5_d',    'E6_y',    'E6_m',\n",
       "          'E6_d',   'E10_y',   'E10_m',   'E10_d',   'E14_y',   'E14_m',\n",
       "         'E14_d',   'E16_y',   'E16_m',   'E16_d',   'E18_y',   'E18_m',\n",
       "         'E18_d',         0,         1,         2,         3,         4,\n",
       "               5,         6,         7,         8,         9,        10,\n",
       "              11,        12,        13,        14,        15,        16,\n",
       "              17,        18,        19],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.to_csv('../data/X_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_true_encoded.to_csv('../data/X_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv('../data/y_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
